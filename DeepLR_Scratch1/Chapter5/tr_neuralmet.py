import numpy as np
import os, sys
sys.path.append("C:\\Users\\Suseong Kim\\Desktop\\MILAB_VENV\\DeepLR_Scratch1\\Chapter4")
sys.path.append("C:\\Users\\Suseong Kim\\Desktop\\MILAB_VENV\\DeepLR_Scratch1\\Chapter3")
sys.path.append("C:\\Users\\Suseong Kim\\Desktop\\MILAB_VENV\\DeepLR_Scratch1\\dataset")
from mnist import load_mnist
from TwoLayerNet_backward import TwoLayerNet
import matplotlib.pyplot as plt
import time
import pickle

(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)

x_val = x_train[50000:]
t_val = t_train[50000:]
x_train = x_train[:50000]
t_train = t_train[:50000]

train_loss_list = []
train_acc_list = []
val_loss_list = []
val_acc_list = []

iters_num = 600
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iter_per_epoch = train_size // batch_size
temp = {}
x = time.time()
tmp_time = 0
time_list = [0, 0, 0, 0, 0, 0, 0]
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    #이전 코드에서 바뀐부분 -> (수치미분 -> 역전파 방식)
    #temp = time.perf_counter()
    grad = network.gradient(x_batch, t_batch, time_list)
    #time_list[0] += time.perf_counter() - temp
    
    if i == iters_num-1:
        temp = grad
        print("W1" + str(network.params["W1"]))
        print("W2" + str(network.params["W2"]))
        print(grad["W1"])
    

    for key in ("W1", "b1", "W2", "b2"):
        network.params[key] -= learning_rate * grad[key]
        
    loss = network.loss(x_batch, t_batch)
    print("train loss: " + str(loss))
    train_loss_list.append(loss)
    if i % 500 == 0:
        val_loss = network.loss(x_val, t_val)
        val_acc = network.accuracy(x_val, t_val)
        train_acc = network.accuracy(x_train, t_train)    
        train_acc_list.append(train_acc)
        val_acc_list.append(val_acc)
        val_loss_list.append(val_loss)
        print("train acc, val acc | " + str(train_acc) + ", " + str(val_acc))
        
network.save_model("two_Layer_net_MNIST.pkl")
model_size = sys.getsizeof(pickle.dumps(network))

print("training time: " + str(time.time() - x))
print("val_acc_list: " + str(val_acc_list))
print("train_acc_list: " + str(train_acc_list))
print("test acc: " + str(network.accuracy(x_test, t_test)))


#print(np.array(time_list) * 1000 / np.array([1201, 1201, 1201, 1201, 1201, 1201, 1201]))
train_loss_avg =[]
a = train_loss_list[:]
for i in range(int(len(a) / (x_train.shape[0] / batch_size))):
    temp = 0
    for _ in range(int(x_train.shape[0] / batch_size)):
        temp += a[0]
        a.pop(0)
    train_loss_avg.append(temp / (x_train.shape[0] / batch_size))
print("train_loss_avg: " + str(train_loss_avg))
print("val_loss_list: " + str(val_loss_list))

plt.figure(figsize=(15, 5))

plt.subplot(131)
x = range(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, val_acc_list, label='val. acc')
plt.legend(loc="upper left")
plt.xlabel("epoch")
plt.ylabel("accuracy")
plt.title("MNIST accuracy")
plt.grid(True)

plt.subplot(132)
plt.plot(x, train_loss_avg, label="train")
plt.plot(x, val_loss_list, label="val.")
plt.legend(loc="upper left")
plt.xlabel("epoch (1 Epoch per 500 iters)")
plt.ylabel("train loss")
plt.title("train/val. Loss")
plt.grid(True)

plt.subplot(133)
plt.plot(x, train_loss_avg, label="train")
plt.plot(x, val_loss_list, label="val.")
plt.legend(loc="upper left")
plt.xlabel("epoch (1 Epoch per 500 iters)")
plt.ylabel("train loss")
plt.ylim(0, 0.5)
plt.title("train/val. Loss-ylim(0, 0.5)")
plt.grid(True)
plt.show()


"""
EPOCH 120
train loss: 0.002621647836351027
training time: 298.5103316307068
val_acc_list: [0.1009, 0.882, 0.9109, 0.9266, 0.9329, 0.9432, 0.9504, 0.9519, 0.9516, 0.9594, 0.9585, 0.9565, 0.9618, 0.9628, 0.9632, 0.9648, 0.9668, 0.9662, 0.9641, 0.9636, 0.9684, 0.9695, 0.9692, 0.969, 0.9711, 0.9683, 0.9709, 0.9717, 0.9707, 0.9715, 0.97, 0.9708, 0.9725, 0.97, 0.9713, 0.9721, 0.9735, 0.9712, 0.9712, 0.9717, 0.9725, 0.9714, 0.9721, 0.9721, 0.9717, 0.972, 0.9727, 0.9724, 0.9724, 0.9722, 0.9735, 0.9735, 0.9736, 0.9731, 0.9732, 0.9742, 0.9735, 0.9723, 0.9746, 0.974, 0.974, 0.9733, 0.9732, 0.9742, 0.9754, 0.9741, 0.9734, 0.9737, 0.9722, 0.9746, 0.9734, 0.9747, 0.9747, 0.9736, 0.9746, 0.9734, 0.9724, 0.9749, 0.9741, 0.9746, 0.975, 0.9748, 0.9745, 0.9746, 0.9754, 0.9755, 0.9739, 0.9755, 0.9755, 0.9753, 0.9747, 0.9748, 0.9748, 0.9757, 0.9747, 0.9741, 0.9751, 0.9751, 0.976, 0.9746, 0.9752, 0.9749, 0.9751, 0.9745, 0.9752, 0.9752, 0.9751, 0.9757, 0.9752, 0.9742, 0.9747, 0.9754, 0.9752, 0.9759, 0.9756, 0.9753, 0.975, 0.9752, 0.975, 0.9753]
train_acc_list: [0.09684, 0.87028, 0.90228, 0.91988, 0.92706, 0.93796, 0.94494, 0.95006, 0.95218, 0.95802, 0.96082, 0.95882, 0.96652, 0.96704, 0.97016, 0.97254, 0.9734, 0.97582, 0.97482, 0.97456, 0.97782, 0.97974, 0.98044, 0.9806, 0.98184, 0.98206, 0.98394, 0.98426, 0.9847, 0.98528, 0.9856, 0.98702, 0.98766, 0.98804, 0.98746, 0.98904, 0.98964, 0.98972, 0.98996, 0.99072, 0.99064, 0.99104, 0.99132, 0.9915, 0.99206, 0.9924, 0.99298, 0.99358, 0.99376, 0.99444, 0.99312, 0.99424, 0.99422, 0.99438, 0.99516, 0.99544, 0.99518, 0.99554, 0.9954, 0.9953, 0.99544, 0.99658, 0.99632, 0.99656, 0.99678, 0.99688, 0.99722, 0.99692, 0.9971, 0.99738, 0.99752, 0.99778, 0.998, 0.99782, 0.99794, 0.99818, 0.99694, 0.9983, 0.99834, 0.99864, 0.99868, 0.99854, 0.99862, 0.99908, 0.999, 0.99898, 0.9989, 0.99904, 0.9992, 0.99928, 0.99934, 0.99916, 0.99942, 0.99944, 0.99952, 0.9994, 0.99958, 0.99946, 0.99954, 0.99966, 0.99974, 0.99972, 0.9996, 0.9997, 0.9997, 0.99978, 0.99986, 0.9996, 0.9998, 0.99984, 0.99982, 0.99984, 0.99986, 0.99982, 0.99982, 0.99992, 0.9999, 0.99988, 0.99994, 0.99992]
test acc: 0.9739
train_loss_avg: [1.2462866105483406, 0.3331388594618967, 0.270910539692337, 0.2273873660453676, 0.19989980152334094, 0.17130175616180157, 0.1577858585839032, 0.13782667708403418, 0.1282316274261091, 0.11607326835530929, 0.10716113854591036, 0.10104787100321483, 0.09345539597694213, 0.08351302080975032, 0.0796759283629613, 0.07679633772115789, 0.07202602734876974, 0.06515744876387827, 0.06215651815759435, 0.05936944967652569, 0.05798167360021465, 0.053116717819049045, 0.04946255867717957, 0.04589121140621501, 0.045467442604340186, 0.04351823321570838, 0.04029193189846328, 0.037676129642979665, 0.037262190422409795, 0.03499054531245782, 0.03593597379442487, 0.0326468906376136, 0.03111556509432585, 0.029975260826667006, 0.02959763811672606, 0.028790527207556482, 0.027105819330367476, 0.02519578681591403, 0.024939434295950073, 0.023790333233989353, 0.02340135813733638, 0.02224246873919586, 0.02111854599602867, 0.019696359051269936, 0.019411112098567565, 0.01882782471325507, 0.019033178987460306, 0.017254119094747382, 0.017393513306538003, 0.017420991679614806, 0.01625767629928932, 0.01631227664082856, 0.016126040506588144, 0.015406001166546219, 0.013790486811521164, 0.012915619229978923, 0.012410123227367717, 0.012710463844633622, 0.01224571701840323, 0.012255574854019879, 0.011539774329845667, 0.011587280561961063, 0.011109523861289796, 0.010651344331910674, 0.009513555995717283, 0.010337962435793656, 0.010400336622154149, 0.009686127023422204, 0.009712440372562277, 0.00933270151387146, 0.008988929386137965, 0.008854942567846228, 0.008758050569322322, 0.008692360148687241, 0.008572762242332933, 0.007900495369102081, 0.007812223066519921, 0.007677451871619763, 0.00730360878362879, 0.006963512958750049, 0.007247536747908498, 
0.007434711168990826, 0.006525943433841973, 0.006677261749905045, 0.006494593961282489, 0.0064205477632297125, 0.006282260005628105, 0.006221855902967909, 0.005969577576750573, 0.005996611193398047, 0.005884945722778223, 0.005684414155392952, 0.00568623236319913, 0.005255454779777458, 0.00529041584523156, 0.005289682443715087, 0.0051726837747791575, 0.0050822369159484095, 
0.0051880563817351295, 0.005015520641018058, 0.004897512853933678, 0.00482132208077187, 0.0048236840703921, 0.004616969736854631, 0.004476114343253727, 0.004404636878719743, 0.0044284359915245574, 0.004345128021345375, 0.004217453811405374, 0.004305643458390905, 0.00402084504223453, 0.004057059333249197, 0.0038960663439072253, 0.003948956815088094, 0.004031179875504621, 0.0038223373995988135, 0.003725911850473588, 0.003687614555147348, 0.0035882697878981454, 0.003723023575494038]
val_loss_list: [2.3025512594134887, 0.4257258652749797, 0.3112285771959847, 0.2585856028012793, 0.23919152590771536, 0.20533797639421336, 0.1865904912405315, 0.17424953310501506, 0.16735429796952012, 0.15297433430524557, 0.14894561307769955, 0.1540870296418103, 0.13562469034585475, 0.13322810483371578, 0.12749040905309067, 0.12368947987371937, 0.1229031568565897, 0.1170056078077125, 0.11930774465932598, 0.12220217196800603, 0.10895180948102368, 0.10806032293485944, 0.10704001378608337, 0.10611732630605777, 0.10484677358358026, 0.1067910797447544, 0.10098906436006276, 0.10117695779531376, 0.10070333873053453, 0.100685044191946, 0.10130492651168635, 0.1007241485258985, 0.09949313558377175, 0.09931710676413336, 0.10085639348141298, 0.09674412423158248, 0.09792380224290383, 0.09932488414919433, 0.09809767976102018, 0.09758962844980182, 0.09670811429225307, 0.09913475307770159, 0.09750859002834499, 0.09839750880345159, 0.09870850203216724, 0.10006733274277504, 0.09835745893718144, 0.0980851792371697, 0.09901753735490074, 0.0976760803557254, 0.09746414175486763, 0.09744212336743653, 0.09767418098632938, 0.09667772554015965, 0.09870118036937717, 0.09750778865138662, 0.09882828591254092, 0.10098558342780439, 0.10025237786220646, 0.09935266390176165, 0.09989518770111531, 0.0982172015127728, 0.1011334284315909, 0.09957313917568347, 0.09810996791022926, 0.10015375516621264, 0.1019971471616693, 0.10120793539520792, 0.1053065812921529, 0.10058591181925794, 0.10481876555557951, 0.10034069621816817, 0.10094867419800284, 0.10362292054360098, 0.10247341222398536, 0.10345866275814815, 0.10799655297200941, 0.1019591005553783, 0.10270507620403037, 0.10405648536190767, 0.1054237052691477, 0.10512956953314183, 0.102745697640046, 0.10517097586170651, 0.10442659676961495, 0.10382736845305848, 0.10562463916451711, 0.10492106732548794, 0.10501629222588933, 0.10642033748399655, 0.10538581256374145, 0.10603138509229709, 0.10621487671233788, 0.10506419941466898, 0.10556354995397385, 0.10546261547472086, 0.10658259554594707, 0.10610951762180652, 0.10584339723116556, 0.10825772538437113, 0.10644547137379189, 0.10767205323869007, 0.10670605852706858, 0.10747314352728804, 0.1073891558160995, 0.10793171696859477, 0.10861671830624949, 0.10763306314861527, 0.10668096409129717, 0.1078733857294172, 0.11029101215398329, 0.10945986199549941, 0.11094860356291134, 0.1083298002642043, 0.1102374691599525, 0.11021523429488889, 0.1107274286337986, 0.11046275389079079, 0.11262386954057786, 0.11121624297652359]
"""

"""
EPOCH 52
train loss: 0.04247646284057055
training time: 137.4457676410675
val_acc_list: [0.1043, 0.8839, 0.9142, 0.9224, 0.9306, 0.9422, 0.9463, 0.9463, 0.9504, 0.9554, 0.9577, 0.9607, 0.9621, 0.9637, 0.9639, 0.9648, 0.9659, 0.9675, 0.9688, 0.9686, 0.9679, 0.9688, 0.9709, 0.9703, 0.9715, 0.969, 0.9727, 0.972, 0.9722, 0.9714, 0.9718, 0.972, 0.972, 0.972, 0.9733, 0.9728, 0.9716, 0.9728, 0.9735, 0.9735, 0.9721, 0.9716, 0.9727, 0.9723, 0.9718, 0.9707, 0.9732, 0.9732, 0.9726, 0.9723, 0.9734, 0.973]
train_acc_list: [0.114, 0.87256, 0.90492, 0.91618, 0.92608, 0.93584, 0.94164, 0.94568, 0.9519, 0.95584, 0.9587, 0.9625, 0.9656, 0.96818, 0.9689, 0.97034, 0.97172, 0.9741, 0.97564, 0.97686, 0.97762, 0.97888, 0.98024, 0.981, 0.98188, 0.98186, 0.98378, 0.98408, 0.98446, 0.98466, 0.98646, 0.98708, 0.98692, 0.98692, 0.98776, 0.9883, 0.98918, 0.98992, 0.98928, 0.99076, 0.99008, 0.99, 0.9913, 0.99178, 0.99224, 0.99248, 0.99276, 0.9928, 0.99264, 0.9929, 0.9943, 0.99424]
test acc: 0.9723
train_loss_avg: [1.2428311808413393, 0.33349494093330045, 0.2743744709141628, 0.2360574314605166, 0.20737159845417566, 0.1812294495413193, 0.16023526626448145, 0.14858181160097161, 0.13338812203904732, 0.12019372351743053, 0.10947734950415766, 0.09608092576036324, 0.09352601744359572, 0.08583213548218177, 0.07847610995745333, 0.0776469013863249, 0.07212270537472043, 0.0640245653604515, 0.062141477023543876, 0.05866498718859239, 0.05854685320657072, 0.05309116746702027, 0.050566746331800964, 0.049315494124335134, 0.04619572064754616, 0.04348608348071219, 0.04034395649211281, 0.038496150272795, 0.04037856335888502, 0.03466019624065902, 0.03464459060796119, 0.03245523191995403, 0.031568203006844035, 0.029959577987596023, 0.028255205061632812, 
0.027612973885435568, 0.025526579022643207, 0.024599729105705265, 0.026129886446689052, 0.024224905504312184, 0.023225229776666405, 0.02171832212342241, 0.021338493484866015, 0.021136638355568543, 0.01989825702393739, 0.01909239722149612, 0.018252887644511634, 0.017432556996260756, 0.016370726434857732, 0.016294519279433713, 0.01594247447653657, 0.01509841964476814]       
val_loss_list: [2.302628883681581, 0.41779675333482374, 0.30672368310215226, 0.27068343328338224, 0.24608930280411137, 0.21362068632296877, 0.2018137799840864, 0.18979471732281858, 0.17401453905709358, 0.16225038423293917, 0.15611938203324613, 0.14311633987672479, 0.13826424144137162, 0.13118196355790177, 0.12996804108401377, 0.12846455337466675, 0.12209347006479611, 0.11883340134741349, 0.11568075738344832, 0.11404961476228948, 0.11448195853262463, 0.11227761918153692, 0.10451074371236714, 0.1076582644745371, 0.10449515827757166, 0.10834915629378868, 0.10216014590769822, 0.10147975498283218, 0.10129570972741941, 0.10216203183084481, 0.09732630403954858, 0.09820886716166018, 0.09784982669021888, 0.09756708859661031, 0.09693775150662531, 0.09734449103969703, 0.09780987801697164, 0.09472041368203148, 0.09635882897571603, 0.09547351243242469, 0.09476328908228521, 0.10076231364730963, 0.0966575161450279, 0.09825677350340886, 0.0969559520765212, 0.09894418659001897, 0.09546045019674992, 0.09452994136054106, 0.09745527734019067, 0.09832676593832994, 0.09431668834289526, 0.09724486633903512]
"""
